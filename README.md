# Financial Prediction Pipeline

A modular data pipeline that collects, analyzes, and predicts stock market trends using financial data, machine learning models, and orchestration with Apache Airflow and Docker.


## Overview

This project aims to:

- Fetch stock and financial news data.

- Compute technical indicators and sentiment scores.

- Train and evaluate machine learning models.

- Schedule, orchestrate, and monitor the entire workflow using Airflow.


## Features

- Data Ingestion: Alphavantage + Yahoo financial news.

- Feature Engineering: stock technical indicators, news sentiment analysis.

- Modeling: neural network using TensorFlow and scikit-learn.

- Orchestration: Apache Airflow for DAG management.

- Environment: Docker Compose for local development and easy later cloud deployment.

- Storage: PostgreSQL as a central data store.


## Project structure

- config: Airflow configuration files, autogenerated.

- dags: Airflow direct acyclic graphs (DAGs), that constitute the structure of a pipeline.

- deploy: cloud deployment scripts.

- logs: Airflow log files, autogenerated.

- models: ML models generated by `scripts/model_training.py`.

- plugins: Airflow plugins.

- scripts: core logic of the pipeline, where the scripts for data ingestion, data analysis and model training live.

- sql: database table definitions, with format `<schema>.<table>.sql`.

- tests: test scripts.

- .env.example: example file of what the ```.env``` file of this project must look like.

- docker-compose.yaml: file providing the architecture of services to run Airflow locally.

- Dockerfile: custom Docker image definition to run this project within Airflow.

- requirements.dev: extra Python packages for development.

- requirements.txt: Python packages for production.


## Getting started

### Prerequisites

- Docker + Docker Compose

- Python 3.9+

- Git

- PostgreSQL 17


### Installation


1. Clone the repository
```sh
git clone https://github.com/manu-novella/financial-prediction.git

cd financial-prediction
```

2. Install Python if not already present.

3. Install Docker and Docker Compose. Installing Docker Desktop is a good way, as it comes with both.

4. Install PostgreSQL 17, and optionally pgAdmin or DBeaver to manage databases with a UI.

5. Get a free API key from [Alphavantage](https://www.alphavantage.co/support/#api-key).



### Configuration

This project uses an ```.env``` file to store certain settings. This file should never be stored in git and therefore it is not present in this repository. It must be created locally, at the root of this project, with the simple name ```.env```. The file ```.env.example``` gives an idea of what the final file must look like.

Here is a brief explanation of what each variable represents:

- FINANCIAL_DB_HOST: name of the database host, which is ```localhost``` or ```host.docker.internal``` depending on whether the scripts are run directly or with Docker. Commented, left here for clarity.

- FINANCIAL_DB_PORT: port to access the database server.

- FINANCIAL_DB_NAME: name given to the database which stores the tables used in this project.

- FINANCIAL_DB_USER: name of the user with permission to access the database. By default, ```postgres```.

- FINANCIAL_DB_PASSWORD: password of such user.

- ASSETS_PRICE_TABLE: ```inputs.asset_prices```.

- ASSETS_TABLE: ```inputs.assets```.

- SENTIMENT_SOURCES_TABLE: ```inputs.sentiment_sources```.

- SENTIMENT_ANALYSIS_TABLE: ```analytics.sentiment_analysis```.

- TECHNICAL_ANALYSIS_TABLE: ```analytics.technical_analysis```.

- FEATURE_MATRIX_TABLE: ```modeling.feature_matrix```.

- ALPHAVANTAGE_API_KEY: API key to access Alphavantage services.

- AIRFLOW__CORE__FERNET_KEY: Fernet key to securely store Airflow secrets. It can be generated in a command-line interface with the command 
```sh
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
```



### Database configuration

To configure the database, the required tables must first be created. For that, open a database interface, through DBeaver, for instance, and run one by one the SQL files in the ```sql``` directory.

There is a single database in this project and it is almost entirely automatic when it comes to being filled with data. However, there exists a configuration table that must be set before the project can be run. This table's name is the one defined in the secret ```ASSETS_TABLE``` in the ```.env``` file, by default named ```inputs.assets```.

This project currently only works with stock market data regarding the SPDR S&P 500 ETF, whose symbol is "SPY". Therefore, the configuration data regarding this asset must be input in the database running the following SQL script:

```sql
INSERT INTO inputs.assets(
	ticker, name, pseudonym, type, alphavantage_code)
	VALUES ('SPY', 'SPDR S&P 500 ETF', 'S&P 500', 'ETF', 'SPY');
```

To expand the capabilities of this project, this table must be updated accordingly. For the column ```alphavantage_code```, consult the [Alphavantage documentation](https://www.alphavantage.co/documentation/).


### Runnning the pipeline

1. Open a command-line interface and go to the root of the project directory.

2. Run ```docker compose build```. This will build the Docker images necessary for this project to run. Be warned, the first time it may take a while.

3. Run ```docker compose up``` to run the generated image. It will start Airflow, accessible by default at ```http://localhost:8080/``` in a browser.

4. Navigate the Airflow web interface and trigger the pipeline.

5. When finished, in the command-line interface Airflow is running, abort the process.

6. Run ```docker compose down``` to shut down the Docker containers.